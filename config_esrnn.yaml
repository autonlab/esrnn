name: M4 quarterly ESRNN Experiment
dataset_name: m4quarterly
architecture: dilated rnn
device: cpu
train_parameters:
  max_epochs: 100 #15
  freq_of_test: 1 #Number of epochs for each test.
  learning_rate: 1e-3 #1e-3
  lr_scheduler_step_size: 10
  per_series_lr_multip: 1.0 #1
  gradient_eps: 1e-6
  gradient_clipping_threshold: 20 #20
  noise_std: 0.001
  numeric_threshold: 1e+38
  level_variability_penalty: 80 # 80 #Multiplier for L" penalty against wigglines of level vector. Important.
  c_state_penalty: 0
  percentile: 50 #we always use Pinball loss, although on normalized values. When forecasting point value, we actually forecast median, so PERCENTILE=50.
  training_percentile: 45 #the program has a tendency for positive bias. So, we can reduce it by running smaller TRAINING_PERCENTILE.
data_parameters:
  seasonality: 30 #
  input_size: 30 # extremely important parameters
  output_size: 30
  exogenous_size: 0
  min_inp_seq_length: 0
  output_dir: "./"
  max_num_series: 5000 #use all series 24000
  frequency: 'D'
model_parameters:
  state_hsize: 40 #40
  lback: 0 #LBACK 0 means final mode: learning on all data and forecasting. LBACK=1 would move back by OUTPUT_SIZE, and forecast last known OUTPUT_SIZE points, for backtesting. LBACK could be a larger integer, but then number of series shrinks.
  dilations: [1, 2, 4, 8] #Each vector represents one chunk of Dilateed LSTMS, connected in standard resnNet fashion
  add_nl_layer: FALSE  #whether to insert a tanh() layer between the RNN stack and the linear adaptor (output) layers