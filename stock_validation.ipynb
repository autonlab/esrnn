{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ESRNN import ESRNN\n",
    "plt.style.use('ggplot')\n",
    "pd.options.display.max_rows = 999\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "def plot_prediction(y, y_hat):\n",
    "    n_y = len(y)\n",
    "    n_yhat = len(y_hat)\n",
    "    ds_y = np.array(range(n_y))\n",
    "    ds_yhat = np.array(range(n_y-n_yhat, n_y))\n",
    "\n",
    "    plt.plot(ds_y, y, label = 'y')\n",
    "    plt.plot(ds_yhat, y_hat, label='y_hat')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "def plot_decomposition(trend, seasonality, level):\n",
    "    ds_y = np.array(range(len(trend)))\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.plot(ds_y, trend, label = 'trend')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.plot(ds_y, seasonality, label='seasonality')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.plot(ds_y, level, label='level')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffill_missing_dates_particular_serie(serie, min_date, max_date, freq):\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
    "    unique_id = serie['unique_id'].unique()\n",
    "    df_balanced = pd.DataFrame({'ds':date_range, 'key':[1]*len(date_range), 'unique_id': unique_id[0]})\n",
    "\n",
    "    # Check balance\n",
    "    check_balance = df_balanced.groupby(['unique_id']).size().reset_index(name='count')\n",
    "    assert len(set(check_balance['count'].values)) <= 1\n",
    "    df_balanced = df_balanced.merge(serie, how=\"left\", on=['unique_id', 'ds'])\n",
    "\n",
    "    df_balanced['y'] = df_balanced['y'].fillna(method='ffill')\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "def ffill_missing_dates_per_serie(df, freq, fixed_max_date=None):\n",
    "    \"\"\"Receives a DataFrame with a date column and forward fills the missing gaps in dates, not filling dates before\n",
    "    the first appearance of a unique key\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input DataFrame\n",
    "    key: str or list\n",
    "        Name(s) of the column(s) which make a unique time series\n",
    "    date_col: str\n",
    "        Name of the column that contains the time column\n",
    "    freq: str\n",
    "        Pandas time frequency standard strings, like \"W-THU\" or \"D\" or \"M\"\n",
    "    numeric_to_fill: str or list\n",
    "        Name(s) of the columns with numeric values to fill \"fill_value\" with\n",
    "    \"\"\"\n",
    "    if fixed_max_date is None:\n",
    "        df_max_min_dates = df[['unique_id', 'ds']].groupby('unique_id').agg(['min', 'max']).reset_index()\n",
    "    else:\n",
    "        df_max_min_dates = df[['unique_id', 'ds']].groupby('unique_id').agg(['min']).reset_index()\n",
    "        df_max_min_dates['max'] = fixed_max_date\n",
    "\n",
    "    df_max_min_dates.columns = df_max_min_dates.columns.droplevel()\n",
    "    df_max_min_dates.columns = ['unique_id', 'min_date', 'max_date']\n",
    "\n",
    "    df_list = []\n",
    "    for index, row in df_max_min_dates.iterrows():\n",
    "        df_id = df[df['unique_id'] == row['unique_id']]\n",
    "        df_id = ffill_missing_dates_particular_serie(df_id, row['min_date'], row['max_date'], freq)\n",
    "        df_list.append(df_id)\n",
    "\n",
    "    df_dates = pd.concat(df_list).reset_index(drop=True).drop('key', axis=1)[['unique_id', 'ds', 'y']]\n",
    "\n",
    "    return df_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/stock/train.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['year'] = data['Date'].dt.year.astype(str)\n",
    "data['unique_id'] = data['Company']+\"_\"+data['year']\n",
    "data = data.rename(columns={'Date':'ds', 'Close':'y'})\n",
    "\n",
    "#Series must be complete in the frequency\n",
    "data = ffill_missing_dates_per_serie(data,'D')\n",
    "data = data.drop_duplicates(['unique_id','ds'])\n",
    "\n",
    "X_train = data[['unique_id','ds']]\n",
    "X_train['x'] = '1'\n",
    "y_train = data[['unique_id','ds','y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('data/stock/test.csv')\n",
    "data_test['ds'] = pd.to_datetime(data_test['Date'])\n",
    "data_test['year'] = data_test['ds'].dt.year.astype(str)\n",
    "data_test['unique_id'] = data_test['Company']+\"_\"+data_test['year']\n",
    "X_test = data_test[['unique_id','ds','Close']]\n",
    "X_test.columns = ['unique_id', 'ds', 'y']\n",
    "uniques = X_test['unique_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "esrnn = ESRNN(max_epochs=100, batch_size=16, learning_rate=3e-4, gradient_clipping_threshold=20,\n",
    "              dilations=[[1, 7], [28]], add_nl_layer=True, per_series_lr_multip=1.5, \n",
    "              seasonality=7, input_size=7, output_size=30, max_periods=12, level_variability_penalty=100)\n",
    "esrnn.fit(X_train, y_train, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = esrnn.predict(X_test, decomposition=True)\n",
    "X_plot = y_train.append(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_id = 3\n",
    "y_test_plot = X_plot.loc[X_plot['unique_id']==uniques[plot_id]]\n",
    "plot_prediction(y_test_plot['y'], y_test_plot['y_hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decomposition(y_test_plot['trend'], y_test_plot['seasonalities'], y_test_plot['level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(y_hat['y_hat']-y_hat['y']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esrnn_torch",
   "language": "python",
   "name": "esrnn_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
