{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ESRNN import ESRNN\n",
    "plt.style.use('ggplot')\n",
    "pd.options.display.max_rows = 999\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "def plot_prediction(y, y_hat):\n",
    "    n_y = len(y)\n",
    "    n_yhat = len(y_hat)\n",
    "    ds_y = np.array(range(n_y))\n",
    "    ds_yhat = np.array(range(n_y-n_yhat, n_y))\n",
    "\n",
    "    plt.plot(ds_y, y, label = 'y')\n",
    "    plt.plot(ds_yhat, y_hat, label='y_hat')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffill_missing_dates_particular_serie(serie, min_date, max_date, freq):\n",
    "    date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
    "    unique_id = serie['unique_id'].unique()\n",
    "    df_balanced = pd.DataFrame({'ds':date_range, 'key':[1]*len(date_range), 'unique_id': unique_id[0]})\n",
    "\n",
    "    # Check balance\n",
    "    check_balance = df_balanced.groupby(['unique_id']).size().reset_index(name='count')\n",
    "    assert len(set(check_balance['count'].values)) <= 1\n",
    "    df_balanced = df_balanced.merge(serie, how=\"left\", on=['unique_id', 'ds'])\n",
    "\n",
    "    df_balanced['y'] = df_balanced['y'].fillna(method='ffill')\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "def ffill_missing_dates_per_serie(df, freq, fixed_max_date=None):\n",
    "    \"\"\"Receives a DataFrame with a date column and forward fills the missing gaps in dates, not filling dates before\n",
    "    the first appearance of a unique key\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input DataFrame\n",
    "    key: str or list\n",
    "        Name(s) of the column(s) which make a unique time series\n",
    "    date_col: str\n",
    "        Name of the column that contains the time column\n",
    "    freq: str\n",
    "        Pandas time frequency standard strings, like \"W-THU\" or \"D\" or \"M\"\n",
    "    numeric_to_fill: str or list\n",
    "        Name(s) of the columns with numeric values to fill \"fill_value\" with\n",
    "    \"\"\"\n",
    "    if fixed_max_date is None:\n",
    "        df_max_min_dates = df[['unique_id', 'ds']].groupby('unique_id').agg(['min', 'max']).reset_index()\n",
    "    else:\n",
    "        df_max_min_dates = df[['unique_id', 'ds']].groupby('unique_id').agg(['min']).reset_index()\n",
    "        df_max_min_dates['max'] = fixed_max_date\n",
    "\n",
    "    df_max_min_dates.columns = df_max_min_dates.columns.droplevel()\n",
    "    df_max_min_dates.columns = ['unique_id', 'min_date', 'max_date']\n",
    "\n",
    "    df_list = []\n",
    "    for index, row in df_max_min_dates.iterrows():\n",
    "        df_id = df[df['unique_id'] == row['unique_id']]\n",
    "        df_id = ffill_missing_dates_particular_serie(df_id, row['min_date'], row['max_date'], freq)\n",
    "        df_list.append(df_id)\n",
    "\n",
    "    df_dates = pd.concat(df_list).reset_index(drop=True).drop('key', axis=1)[['unique_id', 'ds', 'y']]\n",
    "\n",
    "    return df_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/terra/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/terra/train.csv')\n",
    "data['ds'] = pd.to_datetime(data['day'], unit='d')\n",
    "data['unique_id'] = data['cultivar'] + data['sitename']\n",
    "data = data.rename(columns={'canopy_height':'y'})\n",
    "\n",
    "#Series must be complete in the frequency\n",
    "data = ffill_missing_dates_per_serie(data,'D')\n",
    "data = data.drop_duplicates(['unique_id','ds'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[['unique_id','ds']]\n",
    "X_train['x'] = '1'\n",
    "y_train = data[['unique_id','ds','y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = y_train[y_train['unique_id']=='PI157030MAC Field Scanner Season 4 Range 17 Column 14']\n",
    "plt.plot(data_plot['ds'], data_plot['y'], label = 'y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('data/terra/test_real.csv')\n",
    "data_test['ds'] = pd.to_datetime(data_test['day'], unit='d')\n",
    "data_test['unique_id'] = data_test['cultivar'] + data_test['sitename']\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_test[['unique_id','ds','canopy_height']]\n",
    "X_test.columns = ['unique_id', 'ds', 'y']\n",
    "uniques = X_test['unique_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "esrnn = ESRNN(max_epochs=1, batch_size=16, learning_rate=3e-4, gradient_clipping_threshold=20,\n",
    "              dilations=[[1, 7], [28]], add_nl_layer=True, per_series_lr_multip=1.0, \n",
    "              seasonality=7, input_size=7, output_size=50, max_periods=20, level_variability_penalty=80,\n",
    "              rnn_weight_decay=0)\n",
    "esrnn.fit(X_train, y_train, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = esrnn.predict(X_test[['unique_id']])\n",
    "X_plot = y_train.append(y_hat)\n",
    "plot_id = 0\n",
    "y_test_plot = X_plot.loc[X_plot['unique_id']==uniques[plot_id]]\n",
    "plot_prediction(y_test_plot['y'], y_test_plot['y_hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = esrnn.predict(X_test[['unique_id','ds','y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(y_hat['y_hat']-y_hat['y']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoidal fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/terra/train.csv')\n",
    "data['unique_id'] = data['cultivar'] + data['sitename']\n",
    "data = data.rename(columns={'canopy_height':'y', 'day':'t'})\n",
    "data = data.drop_duplicates(['unique_id','t'])\n",
    "X_train = data[['unique_id','t', 'y']]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('data/terra/test_real.csv')\n",
    "data_test['unique_id'] = data_test['cultivar'] + data_test['sitename']\n",
    "data_test = data_test.rename(columns={'canopy_height':'y', 'day':'t'})\n",
    "X_test = data_test[['unique_id','t','y']]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_function(object):\n",
    "    def __init__(self, bounds):\n",
    "        self.bounds = bounds\n",
    "        \n",
    "    def logisticf(self, x, a, b, c):\n",
    "        return  a / (1.0 + np.exp(-b*(x-c)))\n",
    "        \n",
    "    def train(self, X_df):\n",
    "        def logisticf(x, a, b, c):\n",
    "            return a / (1.0 + np.exp(-b*(x-c)))\n",
    "        \n",
    "        for i, serie in enumerate(self.unique_ids):\n",
    "            x = X_df[X_df['unique_id']==serie]['t']\n",
    "            y = X_df[X_df['unique_id']==serie]['y']\n",
    "    \n",
    "            popt, _ = curve_fit(logisticf, x, y, bounds=self.bounds)\n",
    "            self.parameters[i,0] = popt[0]\n",
    "            self.parameters[i,1] = popt[1]\n",
    "            self.parameters[i,2] = popt[2]\n",
    "            self.last_day[i] = max(x)\n",
    "        \n",
    "    def fit(self, X_df):\n",
    "        X_df = X_df.sort_values(['unique_id','t'])\n",
    "\n",
    "        self.unique_ids = X_df['unique_id'].unique()\n",
    "        self.n_series = len(self.unique_ids)\n",
    "        self.parameters = np.zeros((self.n_series, 3))\n",
    "        self.last_day = np.zeros(self.n_series)\n",
    "        \n",
    "        # Train\n",
    "        self.train(X_df)\n",
    "    \n",
    "    def predict(self, X_df, predict_train = False):\n",
    "        X_df = X_df.sort_values(['unique_id','t'])\n",
    "        max_date = X_df['t'].max() + 1\n",
    "        test_ids = X_df['unique_id'].unique()\n",
    "        \n",
    "        Y_hat_panel = pd.DataFrame(columns=['unique_id', 't', 'y_hat'])\n",
    "        \n",
    "        for i, unique_id in enumerate(test_ids):\n",
    "            Y_hat_id = pd.DataFrame(np.zeros(shape=(int(max_date), 1)), columns=[\"y_hat\"]) # -self.last_day[i]\n",
    "                \n",
    "            t_test = np.arange(0, max_date, 1) #self.last_day[i]\n",
    "            y_hat = self.logisticf(t_test, self.parameters[i,0], self.parameters[i,1], self.parameters[i,2])\n",
    "            Y_hat_id.iloc[:, 0] = y_hat\n",
    "            Y_hat_id[\"unique_id\"] = unique_id\n",
    "            Y_hat_id[\"t\"] = t_test\n",
    "            Y_hat_panel = Y_hat_panel.append(Y_hat_id, sort=False).reset_index(drop=True)\n",
    "        \n",
    "        return Y_hat_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = logistic_function(([250, 0, 0], [400, 2, 120]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = X_test.merge(preds,on=['unique_id','t'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(y_hat['y_hat']-y_hat['y']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = X_train.merge(preds,on=['unique_id','t'], how='outer')\n",
    "X_plot = X_plot.merge(X_test,on=['unique_id','t'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = 30\n",
    "x_plot = X_plot[X_plot['unique_id']==uniques[ids]].sort_values('t')\n",
    "x = x_plot['t'].values\n",
    "y_train = x_plot['y_x'].values\n",
    "y_test = x_plot['y_y'].values\n",
    "y_hat = x_plot['y_hat'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y_hat, label = 'prediction')\n",
    "plt.scatter(x, y_train, label = 'train', c ='black')\n",
    "plt.scatter(x, y_test, label = 'test', c ='blue')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_crops = X_plot.copy()\n",
    "test_crops['error'] = test_crops['y_y'] - test_crops['y_hat']\n",
    "test_crops = test_crops[test_crops['error'].notnull()]\n",
    "test_crops = test_crops[['unique_id','t','y_y','y_hat','error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_crops = test_crops.merge(data_test[['unique_id','t','cultivar','sitename']],on=['unique_id','t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_crops[['cultivar','error']].groupby('cultivar')['error'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esrnn_torch",
   "language": "python",
   "name": "esrnn_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
